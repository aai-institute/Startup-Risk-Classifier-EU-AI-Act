{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from large_prompts.master_prompt import master_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_AI_Act_prompt(master_prompt, single_use_case):\n",
    "\n",
    "    additional_format = f\"\"\"\\nDo not give any intros or outros. Respond in plain text string only (no unusual arrays), without any formatting f.e. no bold or ## headings or numbered headings. The following is the AI Use case of a startup you have to classify:\\n\\n{single_use_case}\"\"\"\n",
    "\n",
    "    # print(file_content)  # Output all content\n",
    "    return master_prompt + additional_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_use_case = \"\"\"\n",
    "AI Use Case: Industrial Process Automation,\n",
    "Use Case Description: 3dvisionlabs employs 3D computer vision and artificial intelligence to automate tasks and enhance efficiency in industrial and logistics sectors. Their HemiStereo technology provides real-time depth perception, enabling precise analytics in complex environments. The intended purpose is to streamline industrial processes by automating tasks and reducing inefficiencies. It is deployed in the industry and logistics sectors, including manufacturing and supply chain operations. The system operates with a high level of autonomy to monitor and analyze processes without human intervention. It enhances worker productivity and safety by automating repetitive tasks and providing real-time insights. The system uses 3D depth data and visual information captured by HemiStereo cameras. Industrial operators and managers are the target users who aim to optimize processes. The AI system can adapt to various industrial environments and learn from data to improve performance over time. While the safety-critical nature of the system is moderate, accurate operation is essential to prevent potential hazards in industrial settings.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "\n",
    "# client = ollama.Client()\n",
    "\n",
    "# model = \"llama3.2\" # context length = 131072\n",
    "\n",
    "# input_prompt = prepare_AI_Act_prompt(master_prompt, single_use_case)\n",
    "# options={\n",
    "#     'num_ctx': 100,\n",
    "#     'num_predict': 4096\n",
    "# }\n",
    "\n",
    "# # Send query to the model\n",
    "# response = client.generate(model=model, prompt=\"Hi\", options=options)\n",
    "\n",
    "# print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "client = ollama.Client()\n",
    "\n",
    "# Model is fine-tuned for some other task\n",
    "model = \"gemma3:27b\" # context length = 131072\n",
    "\n",
    "input_prompt = prepare_AI_Act_prompt(master_prompt, single_use_case)\n",
    "options={\n",
    "    'num_ctx': 8192, # Context length. Input tokens + output tokens (generated so far) (total must be less than this value)\n",
    "    'num_predict': 4096 # Infinite output tokens\n",
    "}\n",
    "\n",
    "# Send query to the model\n",
    "response = client.generate(model=model, prompt=input_prompt, options=options)\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "client = ollama.Client()\n",
    "\n",
    "model = \"qwq:32b\"\n",
    "\n",
    "input_prompt = prepare_AI_Act_prompt(master_prompt, single_use_case)\n",
    "options={\n",
    "    'num_ctx': 8192, # Context length. Input tokens + output tokens (generated so far) (total must be less than this value)\n",
    "    'num_predict': 4096 # Infinite output tokens\n",
    "}\n",
    "\n",
    "# Send query to the model\n",
    "response = client.generate(model=model, prompt=input_prompt, options=options)\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "client = ollama.Client()\n",
    "\n",
    "model = \"deepseek-r1:32b\"\n",
    "input_prompt = prepare_AI_Act_prompt(master_prompt, single_use_case)\n",
    "options={\n",
    "    'num_ctx': 8192,\n",
    "    'num_predict': 4096\n",
    "}\n",
    "\n",
    "# Send query to the model\n",
    "response = client.generate(model=model, prompt=input_prompt, options=options)\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "client = ollama.Client()\n",
    "\n",
    "model = \"qwen2.5:32b\"\n",
    "input_prompt = prepare_AI_Act_prompt(master_prompt, single_use_case)\n",
    "options={\n",
    "    'num_ctx': 8192,\n",
    "    'num_predict': 4096\n",
    "}\n",
    "\n",
    "# Send query to the model\n",
    "response = client.generate(model=model, prompt=input_prompt, options=options)\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "client = ollama.Client()\n",
    "\n",
    "model = \"command-r:35b\"\n",
    "input_prompt = prepare_AI_Act_prompt(master_prompt, single_use_case)\n",
    "options={\n",
    "    'num_ctx': 8192,\n",
    "    'num_predict': 4096\n",
    "}\n",
    "\n",
    "# Send query to the model\n",
    "response = client.generate(model=model, prompt=input_prompt, options=options)\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "client = ollama.Client()\n",
    "\n",
    "model = \"mistral-small:24b\"\n",
    "input_prompt = prepare_AI_Act_prompt(master_prompt, single_use_case)\n",
    "options={\n",
    "    'num_ctx': 8192,\n",
    "    'num_predict': 4096\n",
    "}\n",
    "\n",
    "# Send query to the model\n",
    "response = client.generate(model=model, prompt=input_prompt, options=options)\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suspicious model\n",
    "\n",
    "import ollama\n",
    "\n",
    "client = ollama.Client()\n",
    "\n",
    "model = \"yi:34b\"\n",
    "input_prompt = prepare_AI_Act_prompt(master_prompt, single_use_case)\n",
    "options={\n",
    "    'num_ctx': 8192,\n",
    "    'num_predict': 4096\n",
    "}\n",
    "\n",
    "# Send query to the model\n",
    "response = client.generate(model=model, prompt=input_prompt, options=options)\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "client = ollama.Client()\n",
    "\n",
    "model = \"exaone-deep:32b\"\n",
    "input_prompt = prepare_AI_Act_prompt(master_prompt, single_use_case)\n",
    "options={\n",
    "    'num_ctx': 8192,\n",
    "    'num_predict': 4096\n",
    "}\n",
    "\n",
    "# Send query to the model\n",
    "response = client.generate(model=model, prompt=input_prompt, options=options)\n",
    "\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import itertools\n",
    "\n",
    "def ollama_generate_classification(model, single_use_case):\n",
    "    client = ollama.Client()\n",
    "\n",
    "    input_prompt = prepare_AI_Act_prompt(master_prompt, single_use_case)\n",
    "    options={\n",
    "        'num_ctx': 8192,\n",
    "        'num_predict': 4096\n",
    "    }\n",
    "\n",
    "    # Send query to the model\n",
    "    response = client.generate(model=model, prompt=input_prompt, options=options)\n",
    "\n",
    "    return response.response\n",
    "\n",
    "target_tags = [\n",
    "    \"AI Use Case\",\n",
    "    \"Use Case Description\",\n",
    "    \"Risk Classification\",\n",
    "    \"Reason\",\n",
    "    \"Requires Additional Information\",\n",
    "    \"What additional Information\"\n",
    "]\n",
    "\n",
    "def generate_tag_variants(tag):\n",
    "    \"\"\"Generate all variants by replacing spaces with '', '-' or keeping space.\"\"\"\n",
    "    parts = tag.split(\" \")\n",
    "    combinations = list(itertools.product([\"\", \"-\", \" \"], repeat=len(parts)-1))\n",
    "    variants = set()\n",
    "\n",
    "    for combo in combinations:\n",
    "        variant = parts[0]\n",
    "        for sep, part in zip(combo, parts[1:]):\n",
    "            variant += sep + part\n",
    "        variants.add(variant)\n",
    "    return variants\n",
    "\n",
    "# Map correct tags to all possible variants\n",
    "tag_variants = {tag: generate_tag_variants(tag) for tag in target_tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "with open('use_cases.json', 'r') as file:\n",
    "    use_cases_data = json.load(file)\n",
    "\n",
    "model_list = ['gemma3:27b', 'qwen2.5:32b']\n",
    "\n",
    "for index, company in enumerate(use_cases_data['companies']):\n",
    "    # print(f\"Company: {company['company_name']}\")\n",
    "    for use_case in company[\"use_cases\"]:\n",
    "        single_use_case = f\"\"\"AI Use Case: {use_case[\"use_case_name\"]}\\nUse Case Description: {use_case[\"use_case_description\"]}\"\"\"        \n",
    "        # print(single_use_case + \"\\n\")\n",
    "    \n",
    "        # Run through all models\n",
    "        classification_string = \"\"\n",
    "        for model in model_list:\n",
    "            classification = ollama_generate_classification(model, single_use_case)\n",
    "            # classification = \"\"\n",
    "            classification_string += f\"{classification}\\n\\n########END OF USE CASE########\\n\\n\"\n",
    "            print(classification)\n",
    "\n",
    "#         classification_string = \"\"\"\n",
    "# AI Use Case: Obstacle Detection\n",
    "# Use Case Description: The obstacle detection system is designed to enhance train safety by identifying potential hazards on or near the tracks. It is deployed in the rail transportation sector and is specifically used in both passenger and freight operations. This system has a medium level of autonomy, assisting drivers rather than replacing them. It significantly improves safety by reducing the risk of collisions and enabling timely responses to obstacles. It uses data from camera and LiDAR sensors to detect objects, and the users are primarily train operators and traffic controllers. The system likely incorporates adaptive learning to improve detection accuracy over time and may operate under safety-critical conditions, especially in environments with high traffic or poor visibility.\n",
    "# Risk Classification: High-risk AI system under Annex I\n",
    "# Reason: The AI system is a safety component of a product (trains) covered by Union harmonization laws. Specifically, the Rail System Interoperability Directive (2016/797) applies to rail transportation systems. As a safety component directly impacting train safety and potentially preventing collisions, it falls under the definition of a safety component. Because the system is used for safety-critical operations and is a safety component of a product covered under Union Harmonization laws, it requires third-party conformity assessment under the Rail System Interoperability Directive, making it a High-risk AI system under Annex I. There are no transparency obligations triggered by this use case.\n",
    "# Requires Additional Information: No\n",
    "# What additional Information: asdasdxzczxc- zxczxc\n",
    "\n",
    "# ########END OF USE CASE########\n",
    "\n",
    "# <think>\n",
    "\n",
    "# Okay, let me try to work through this step by step. The AI use case is Obstacle Detection for trains. The description says it enhances safety by identifying hazards on or near tracks, used in rail transport for both passenger and freight. It has medium autonomy, assisting drivers but not replacing them. Uses camera and LiDAR data, adaptive learning, and operates under safety-critical conditions.\n",
    "\n",
    "# First, check if it's a prohibited system under any of the articles A to H. Let's go through each:\n",
    "\n",
    "# A) Harmful Manipulation and Deception: Doesn't involve manipulation or deception. It's about detecting obstacles, so no here.\n",
    "\n",
    "# B) Exploitation of Vulnerabilities: Not targeting any vulnerable groups like age or disability. The system is for physical safety, so no.\n",
    "\n",
    "# C) Social Scoring: No, it's not evaluating people's behavior or traits for social scoring.\n",
    "\n",
    "# D) Crime Risk Assessment: Not assessing individuals' crime risk. It's about obstacles, not criminal behavior.\n",
    "\n",
    "# E) Untargeted Facial Scraping: Not related to facial recognition, so no.\n",
    "\n",
    "# F) Emotion Recognition: Not inferring emotions, so no.\n",
    "\n",
    "# G) Biometric Categorization: Doesn't categorize individuals based on biometric data like race or religion.\n",
    "\n",
    "# H) Real-time Remote Biometric Identification: Not using biometric data for identification in public spaces by law enforcement. It's about obstacles, so no.\n",
    "\n",
    "# So, not prohibited. Next, check if it's a high-risk under Annex I. The criteria are that it's a product or safety component covered by Union harmonization laws and requires third-party assessment. \n",
    "\n",
    "# The system is part of rail transportation. Looking at the Union harmonization laws in Annex I sections. Section B includes Rail System Interoperability Directive (2016/797). Since the system is a safety component of a rail system (as it's for obstacle detection in trains), it's part of the product covered under that directive. Safety components are those that fulfill a safety function or whose failure would endanger safety. Since obstacle detection is a safety function, it qualifies. Now, does the directive require third-party conformity assessment? The user instruction says if it's under Annex I and the product requires third-party assessment under that law. Assuming rail systems are covered (since they are in the list), then this would be Annex I. \n",
    "\n",
    "# Next, check if it's high-risk under Annex III. The categories in Annex III include critical infrastructure (section b). The system is part of rail traffic management, which is listed under critical infrastructure. So it could be here too. But since it's already under Annex I, which takes precedence? The user instructions say to check both, but the classification is separate. The user says if it's under either Annex I or III, but the question is which one applies first. Since Annex I applies if it meets those two conditions, which it does, then it's under Annex I. \n",
    "\n",
    "# Now, check transparency obligations. The system interacts with natural persons (train operators) but it's obvious it's an AI system (since it's part of the train's safety system, not pretending to be human). So Art 50(1) exception applies. It doesn't generate synthetic content, use emotion recognition, or biometric categorization. So no transparency obligations. \n",
    "\n",
    "# Therefore, it should be High-risk under Annex I. \n",
    "\n",
    "# Wait, but the system is a safety component of a product covered by the Rail Directive. So the first condition is met. The second condition is that the product or component requires third-party assessment under the directive. Since rail systems are subject to type approval and conformity assessments, this likely holds. Thus, High-risk under Annex I. No transparency obligations because the exceptions apply (obvious interaction, not synthetic content, etc.). So the classification is \"High-risk AI system under Annex I\".\n",
    "# </think>\n",
    "\n",
    "# AI Use Case: Obstacle Detection  \n",
    "# Use Case Description: The obstacle detection system is designed to enhance train safety by identifying potential hazards on or near the tracks. It is deployed in the rail transportation sector and is specifically used in both passenger and freight operations. This system has a medium level of autonomy, assisting drivers rather than replacing them. It significantly improves safety by reducing the risk of collisions and enabling timely responses to obstacles. It uses data from camera and LiDAR sensors to detect objects, and the users are primarily train operators and traffic controllers. The system likely incorporates adaptive learning to improve detection accuracy over time and may operate under safety-critical conditions, especially in environments with high traffic or poor visibility.  \n",
    "# Risk Classification: High-risk AI system under Annex I  \n",
    "# Reason:  \n",
    "# 1. **Prohibited AI System Check**: None of the prohibited categories (A–H) apply. The system does not involve manipulation, deception, exploitation of vulnerabilities, social scoring, crime risk assessment, facial scraping, emotion recognition, biometric categorization, or real-time biometric identification.  \n",
    "# 2. **High-Risk Under Annex I**:  \n",
    "# - The system is a **safety component** of rail transportation systems covered under the **Rail System Interoperability Directive (2016/797)** (Section B of Annex I).  \n",
    "# - It fulfills a safety function (obstacle detection to prevent collisions) and its failure would endanger human safety.  \n",
    "# - Rail systems are subject to third-party conformity assessments under Union harmonization laws (e.g., Directive 2016/797), satisfying both conditions for Annex I classification.  \n",
    "# 3. **High-Risk Under Annex III**: Not applicable. While rail traffic management could fall under critical infrastructure (Annex III, Section b), the system already qualifies under Annex I.  \n",
    "# 4. **Transparency Obligations**: None apply. The system does not interact with natural persons in a deceptive manner (Art 50(1) exception applies as its purpose is obvious), generate synthetic content, use emotion/biometric categorization, or produce deep fakes.  \n",
    "# Requires Additional Information: No  \n",
    "# What additional Information: N/A\n",
    "\n",
    "\n",
    "# ########END OF USE CASE########\n",
    "#         \"\"\"\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        # --- Get the classifications ---\n",
    "        # === Step 3: Normalize headers with optional leading whitespace ===\n",
    "        for correct, variants in tag_variants.items():\n",
    "            for variant in variants:\n",
    "                if variant == correct:\n",
    "                    continue\n",
    "                classification_string = re.sub(\n",
    "                    rf\"^[ \\t]*{re.escape(variant)}\\s*:\", f\"{correct}:\", classification_string, flags=re.MULTILINE\n",
    "                )\n",
    "\n",
    "        # === Step 4: Extract use case blocks ===\n",
    "        use_case_blocks = re.split(r\"########END OF USE CASE########\", classification_string.strip())\n",
    "\n",
    "        # === Step 5: Extract relevant fields ===\n",
    "        split_pattern = re.compile(\n",
    "            rf\"^[ \\t]*({'|'.join(re.escape(tag) for tag in target_tags)})\\s*:\",\n",
    "            flags=re.MULTILINE\n",
    "        )\n",
    "\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for block in use_case_blocks:\n",
    "            block = block.strip()\n",
    "\n",
    "            if not re.search(r\"^AI Use Case:\", block, flags=re.MULTILINE):\n",
    "                continue  # Skip invalid blocks\n",
    "\n",
    "            chunks = split_pattern.split(block)\n",
    "\n",
    "            data = {\n",
    "                \"AI Use Case\": use_case[\"use_case_name\"],\n",
    "                \"Use Case Description\": use_case[\"use_case_description\"],\n",
    "                \"Risk Classification\": \"\",\n",
    "                \"Reason\": \"\",\n",
    "                \"Requires Additional Information\": \"No\",\n",
    "                \"What additional Information\": \"\"\n",
    "            }\n",
    "\n",
    "            for i in range(1, len(chunks), 2):\n",
    "                tag = chunks[i].strip()\n",
    "                value = chunks[i + 1].strip()\n",
    "                if tag in data:\n",
    "                    data[tag] = value\n",
    "\n",
    "            if data[\"Requires Additional Information\"].lower() == \"no\":\n",
    "                data[\"What additional Information\"] = \"\"\n",
    "\n",
    "            results.append(data)\n",
    "\n",
    "\n",
    "        # === Step 6: Output result ===\n",
    "        # print(json.dumps(results, indent=4, ensure_ascii=False))\n",
    "        # Optional: Save to file\n",
    "        with open(\"test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        break\n",
    "    \n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    if index == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time taken = 10 mins\n",
    "model_list = ['gemma3:27b', 'qwq:32b', 'deepseek-r1:32b', 'qwen2.5:32b', 'command-r:35b', 'mistral-small:24b', 'yi:34b', 'exaone-deep:32b']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Tested Individually -->\n",
    "gemma = 35s\n",
    "qwq:32b = 1m40s\n",
    "deepseek-r1-32b = 1m6s\n",
    "qwen2.5-32b = 1m10s\n",
    "command-r:35b = 1m10s\n",
    "mistral-small-24b = 53.5s\n",
    "yi:34b = 1m12s\n",
    "exaone-deep:32b = 3m40s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
