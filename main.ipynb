{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 16\n"
     ]
    }
   ],
   "source": [
    "# Install tiktoken if not already installed: pip install tiktoken\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    # Initialize the tokenizer for the model\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    # Encode the text into tokens\n",
    "    tokens = encoding.encode(text)\n",
    "    # Return the number of tokens\n",
    "    return len(tokens)\n",
    "\n",
    "# Example usage:\n",
    "text = \"https://findiq.de/loesung/vorteile\"\n",
    "text = \" \".join([text] * 100)\n",
    "text = \"\"\"\"<|user|>\n",
    "You are a helpful assistant.\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "# text = \" \".join(['https://findiq.de/', 'https://findiq.de/loesung/vorteile', 'https://findiq.de/loesung/leistungen', 'https://findiq.de/loesung/faq', 'https://findiq.de/referenzen/referenzen', 'https://findiq.de/ueber-uns', 'https://findiq.de/news-events', 'https://findiq.de/jobs', 'https://findiq.de/kontakt', 'https://findiq.de/en/', 'https://findiq.de/newsletter', 'https://findiq.de/impressum', 'https://findiq.de/datenschutz', 'https://findiq.de/agb'])\n",
    "# print(text)\n",
    "\n",
    "model_name = \"gpt-4o\"\n",
    "token_count = count_tokens(text, model=model_name)\n",
    "print(f\"Number of tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate characters for 4 tokens: 16\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_chars(num_tokens, avg_chars_per_token=4):\n",
    "    return int(num_tokens * avg_chars_per_token)\n",
    "\n",
    "num_tokens = 4\n",
    "chars = tokens_to_chars(num_tokens)\n",
    "print(f\"Approximate characters for {num_tokens} tokens: {chars}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact characters for 50 tokens: 258\n"
     ]
    }
   ],
   "source": [
    "def tokens_to_characters_exact(num_tokens, model=\"gpt-4o\"):\n",
    "    # Initialize the tokenizer for the model\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    # Generate a dummy text of sufficient length to extract tokens\n",
    "    dummy_text = \"This is an example text to calculate the number of tokens.This is an example text to calculate the number of tokens.This is an example text to calculate the number of tokens.This is an example text to calculate the number of tokens.This is an example text to calculate the number of tokens.This is an example text to calculate the number of tokens.This is an example text to calculate the number of tokens.This is an example text to calculate the number of tokens.This is an example text to calculate the number of tokens.\"\n",
    "    \n",
    "    # Encode the dummy text into tokens\n",
    "    token_ids = encoding.encode(dummy_text)\n",
    "    \n",
    "    # Ensure we have enough tokens for the given number\n",
    "    if len(token_ids) < num_tokens:\n",
    "        raise ValueError(\"Not enough tokens in the dummy text to fulfill the request.\")\n",
    "    \n",
    "    # Decode the specified number of tokens back into text\n",
    "    decoded_text = encoding.decode(token_ids[:num_tokens])\n",
    "    \n",
    "    # Return the length of the decoded text in characters\n",
    "    return len(decoded_text)\n",
    "\n",
    "# Example usage\n",
    "num_tokens = 50\n",
    "exact_chars = tokens_to_characters_exact(num_tokens)\n",
    "print(f\"Exact characters for {num_tokens} tokens: {exact_chars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is the content of the homepage of a company's website whose supposed name is \"Findiq\", but confirm this from the homepage content. \n",
      "\n",
      "Your task is to generate all the AI use cases this company is implementing. You can also use your own knowledge to help with this task. Do not guess any use case, only find the ones that the company is actually implementing. If the content of this webpage is just a page error, then output 'Page Error' only. \n",
      "\n",
      "Content of the homepage: This is some content\n"
     ]
    }
   ],
   "source": [
    "def prompts(startupName, raw_text):\n",
    "    startup_summary = f\"The following is the content of the homepage of a company's website whose supposed name is \\\"{startupName}\\\", but confirm this from the homepage content. \\n\\nYour task is to generate all the AI use cases this company is implementing. You can also use your own knowledge to help with this task. Do not guess any use case, only find the ones that the company is actually implementing. If the content of this webpage is just a page error, then output 'Page Error' only. \\n\\nContent of the homepage: {raw_text}\"\n",
    "\n",
    "    return startup_summary\n",
    "\n",
    "print(prompts(\"Findiq\", \"This is some content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Obtaining dependency information for openpyxl from https://files.pythonhosted.org/packages/c0/da/977ded879c29cbd04de313843e76868e6e13408a94ed6b987245dc7c8506/openpyxl-3.1.5-py2.py3-none-any.whl.metadata\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Obtaining dependency information for et-xmlfile from https://files.pythonhosted.org/packages/c1/8b/5fe2cc11fee489817272089c4203e679c63b570a5aaeb18d852ae3cbba6a/et_xmlfile-2.0.0-py3-none-any.whl.metadata\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='https', netloc='epignostix.com', path='/', params='', query='', fragment='')\n",
      "https://epignostix.com/\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urlunparse, urljoin\n",
    "\n",
    "\n",
    "parsed_url = urlparse(\"https://epignostix.com/\")\n",
    "cleaned_url = urlunparse((parsed_url.scheme, parsed_url.netloc, parsed_url.path, '', '', ''))\n",
    "\n",
    "print(parsed_url)\n",
    "print(cleaned_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_domain(url):\n",
    "        # Extract the netloc (Remove the port number, if any)\n",
    "        parsed_url = urlparse(url)\n",
    "        netloc = parsed_url.netloc.split(\":\")[0] \n",
    "        \n",
    "        # Remove 'www.' if present\n",
    "        if netloc.startswith(\"www.\"):\n",
    "            netloc = netloc[4:]\n",
    "\n",
    "        return netloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epignostix.com\n"
     ]
    }
   ],
   "source": [
    "print(get_base_domain(cleaned_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List 1: https://epignostix.com/our-technology/\n",
      "List 2: https://epignostix.com/news/\n",
      "List 3: https://epignostix.com/about-us/\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def extract_single_list(input_string):\n",
    "    # Regular expression to match Python-style lists\n",
    "    list_pattern = r\"\\[.*?\\]\"\n",
    "    # Find the first match of the pattern in the input string\n",
    "    match = re.search(list_pattern, input_string)\n",
    "    if match:\n",
    "        # Convert the matched string to a Python list using ast.literal_eval\n",
    "        return ast.literal_eval(match.group())\n",
    "    return None  # Return None if no match is found\n",
    "\n",
    "# Example usage\n",
    "input_string = \"['https://epignostix.com/our-technology/', 'https://epignostix.com/news/', 'https://epignostix.com/about-us/']\"\n",
    "extracted_lists = extract_single_list(input_string)\n",
    "\n",
    "for i, lst in enumerate(extracted_lists, start=1):\n",
    "    print(f\"List {i}: {lst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = [1, 2, 3, 4, 5]\n",
    "nums[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = [nums, [6,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nums.extend(other))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = ['', '']\n",
    "text = f\"\\n\\n\".join(array)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The following are all the links available on the homepage of a company's website. Use your best judgement to determine a maximum of 3 links which are most important to find all the AI use cases this company is implementing. Return only a list of links and no other text. If you can not find any relavant links, then return an empty list only.\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_USE_CASES = 4\n",
    "f\"The following are all the links available on the homepage of a company's website. Use your best judgement to determine a maximum of {TOTAL_USE_CASES - 1} links which are most important to find all the AI use cases this company is implementing. Return only a list of links and no other text. If you can not find any relavant links, then return an empty list only.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "# Save the results to a new Excel file\n",
    "output_wb = openpyxl.Workbook()\n",
    "output_sheet = output_wb.active\n",
    "output_sheet.title = \"AI Use Cases\"\n",
    "\n",
    "print(output_sheet.max_row)\n",
    "\n",
    "\n",
    "headers = [\"Startup Name\", \"Homepage URL\", \"Additional URLs\"] + [f\"AI Use Case {i+1}\" for i in range(4)]\n",
    "# Write headers if not present\n",
    "print(output_sheet.max_row)\n",
    "if output_sheet.max_row <= 1:\n",
    "    for col_num, header in enumerate(headers, start=1):\n",
    "        output_sheet.cell(row=1, column=col_num, value=header)\n",
    "\n",
    "# Ensure all_ai_use_cases is padded to match the maximum number of columns\n",
    "use_cases_padded = [\"asdasd\"] + [\"\"] * (4 - len([\"asdasd\"]))\n",
    "\n",
    "# Write data\n",
    "row = [\"startup_name\", \"homepage_url\", \", \".join(\"additional_urls\")] + use_cases_padded[:4]\n",
    "output_sheet.append(row)\n",
    "\n",
    "print(output_sheet.max_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://epignostix.com/our-technology/',\n",
       " 'https://epignostix.com/news/',\n",
       " 'https://epignostix.com/about-us/']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "def extract_list(input_string):\n",
    "    # Regular expression to match Python-style lists\n",
    "    list_pattern = r\"\\[.*?\\]\"\n",
    "    match = re.search(list_pattern, input_string)\n",
    "    if match:\n",
    "        # Convert the matched string to a Python list\n",
    "        return ast.literal_eval(match.group())\n",
    "    return None\n",
    "\n",
    "\n",
    "text = extract_list(\"['https://epignostix.com/our-technology/', 'https://epignostix.com/news/', 'https://epignostix.com/about-us/']\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://epignostix.com/our-technology/ https://epignostix.com/news/ https://epignostix.com/about-us/\n"
     ]
    }
   ],
   "source": [
    "# print(type(str(text)))\n",
    "# str(text)\n",
    "text_str = \" \".join(text)\n",
    "print(text_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Risk Categories',\n",
       " 'Classify AI systems into one of four risk categories:',\n",
       " 'Unacceptable Risk (Prohibited)']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"You are an AI expert tasked with performing risk classification assessments for AI startups and their use cases according to the EU AI Act. Your goal is to categorize AI systems into the appropriate risk level based on the information provided. Here's how to conduct the assessment:\n",
    "Risk Categories\n",
    "Classify AI systems into one of four risk categories:\n",
    "Unacceptable Risk (Prohibited)\n",
    "High Risk\n",
    "Limited Risk\n",
    "Minimal Risk\n",
    "Category Definitions and Examples\n",
    "1. Unacceptable Risk (Prohibited)\n",
    "AI systems in this category are banned in the EU due to their potential threat to individuals and society. These include:\n",
    "Social scoring systems by governments\n",
    "AI manipulating vulnerable groups\n",
    "Real-time and remote biometric identification (e.g., facial recognition) in public spaces, except for specific law enforcement purposes with court approval\n",
    "AI systems using subliminal or manipulative techniques to distort behavior\n",
    "AI exploiting vulnerabilities related to age, disability, or socio-economic circumstances\n",
    "Biometric categorization systems inferring sensitive attributes (e.g., race, political opinions, religious beliefs, sexual orientation)\n",
    "AI for assessing an individual's risk of committing criminal offenses\n",
    "2. High Risk\n",
    "AI systems that could significantly impact safety, fundamental rights, or the environment. The following categories are to be considered high risk:\n",
    "\n",
    "Biometrics:\n",
    "Remote biometric identification systems (excluding verification of identity claims).\n",
    "Biometric categorization based on sensitive attributes.\n",
    " Emotion recognition systems.\n",
    "Critical Infrastructure:\n",
    "AI used as safety components in digital infrastructure, road traffic, or utilities (water, gas, heating, electricity).\n",
    "Education and Vocational Training:\n",
    "AI for admission or placement decisions.\n",
    "AI to evaluate learning outcomes or guide learning processes.\n",
    "AI assessing education access levels.\n",
    "AI monitoring prohibited behavior during tests.\n",
    "Employment and Workforce Management:\n",
    "AI for recruitment, filtering applications, or evaluating candidates.\n",
    "AI for decisions on work contracts, promotions, terminations, task allocations, or performance monitoring.\n",
    "Access to Essential Services:\n",
    "AI assessing eligibility for public benefits/services (e.g., healthcare).\n",
    "AI evaluating creditworthiness (excluding fraud detection).\n",
    "AI for risk assessment in life/health insurance pricing.\n",
    "AI classifying emergency calls or dispatching emergency services.\n",
    "Law Enforcement:\n",
    "AI assessing risk of victimization.\n",
    " AI as polygraphs or similar tools.\n",
    " AI evaluating evidence reliability.\n",
    " AI assessing reoffending risk (not solely based on profiling).\n",
    "AI for profiling during criminal investigations.\n",
    "Migration, Asylum, and Border Control:\n",
    "AI as polygraphs or similar tools.\n",
    "AI assessing security, migration, or health risks.\n",
    "AI for asylum, visa, or residence permit applications.\n",
    "AI for detecting or identifying persons in migration contexts (excluding travel document verification).\n",
    "Justice and Democratic Processes:\n",
    "AI assisting judicial authorities with legal interpretation or dispute resolution.\n",
    "AI influencing election/referendum outcomes or voting behavior (excluding backend campaign management tools).\n",
    "Common Examples:\n",
    "AI powered recruitment and development tools for human resources\n",
    "Legal AI assistants \n",
    "\n",
    "\n",
    "\n",
    "Note: An AI system listed in Annex III may not be considered high-risk if it meets certain conditions, such as improving previously completed human activity, or detecting decision-making patterns without replacing human assessment.\n",
    "3. Limited Risk\n",
    "AI systems with potential for manipulation or deceit, requiring transparency. General purpose AI (GPAI) models also fall under this category. ‘General-purpose AI model’ means an AI model trained on large-scale data, often using self-supervision, that exhibits broad versatility, can perform diverse tasks competently, and is integrable into various downstream systems or applications. This excludes models used solely for research, development, or prototyping prior to market placement.\n",
    "Systemic Risk?\n",
    "Examples:\n",
    "Chatbots\n",
    "Deepfakes\n",
    "4. Minimal Risk\n",
    "AI systems not falling into the above categories, which are largely unregulated.\n",
    "Examples:\n",
    "AI-enabled video games\n",
    "Spam filters\n",
    "Recommender Systems\n",
    "\n",
    "Assessment Process\n",
    "Review the AI startup's technology and use cases.\n",
    "Compare the AI system's purpose and functionality to the examples and definitions provided for each risk category.\n",
    "Consider the potential impact on safety, fundamental rights, and the environment.\n",
    "Classify the AI system into the appropriate risk category.\n",
    "Provide a brief explanation for the classification, referencing specific aspects of the AI Act and examples given.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an AI expert tasked with performing risk classification assessments for AI startups and their use cases according to the EU AI Act. Your goal is to categorize AI systems into the appropriate risk level based on the information provided. Here's how to conduct the assessment:\\n        Risk Categories\\n        Classify AI systems into one of four risk categories:\\n        Unacceptable Risk (Prohibited)\\n        High Risk\\n        Limited Risk\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"You are an AI expert tasked with performing risk classification assessments for AI startups and their use cases according to the EU AI Act. Your goal is to categorize AI systems into the appropriate risk level based on the information provided. Here's how to conduct the assessment:\n",
    "Risk Categories\n",
    "Classify AI systems into one of four risk categories:\n",
    "Unacceptable Risk\"\"\"\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
